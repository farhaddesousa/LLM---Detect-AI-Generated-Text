{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting transformers\n",
      "  Downloading transformers-4.45.2-py3-none-any.whl.metadata (44 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.4/44.4 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: filelock in /home1/fdesousa/.local/lib/python3.10/site-packages (from transformers) (3.16.1)\n",
      "Collecting huggingface-hub<1.0,>=0.23.2 (from transformers)\n",
      "  Downloading huggingface_hub-0.26.0-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: numpy>=1.17 in /home1/fdesousa/.local/lib/python3.10/site-packages (from transformers) (2.1.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /spack/conda/miniforge3/24.3.0/lib/python3.10/site-packages (from transformers) (24.0)\n",
      "Collecting pyyaml>=5.1 (from transformers)\n",
      "  Downloading PyYAML-6.0.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.1 kB)\n",
      "Collecting regex!=2019.12.17 (from transformers)\n",
      "  Downloading regex-2024.9.11-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (40 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.5/40.5 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: requests in /spack/conda/miniforge3/24.3.0/lib/python3.10/site-packages (from transformers) (2.31.0)\n",
      "Collecting safetensors>=0.4.1 (from transformers)\n",
      "  Downloading safetensors-0.4.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.8 kB)\n",
      "Collecting tokenizers<0.21,>=0.20 (from transformers)\n",
      "  Downloading tokenizers-0.20.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
      "Requirement already satisfied: tqdm>=4.27 in /spack/conda/miniforge3/24.3.0/lib/python3.10/site-packages (from transformers) (4.66.2)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /home1/fdesousa/.local/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (2024.9.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home1/fdesousa/.local/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (4.12.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /spack/conda/miniforge3/24.3.0/lib/python3.10/site-packages (from requests->transformers) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /spack/conda/miniforge3/24.3.0/lib/python3.10/site-packages (from requests->transformers) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /spack/conda/miniforge3/24.3.0/lib/python3.10/site-packages (from requests->transformers) (2.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /spack/conda/miniforge3/24.3.0/lib/python3.10/site-packages (from requests->transformers) (2024.2.2)\n",
      "Downloading transformers-4.45.2-py3-none-any.whl (9.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.9/9.9 MB\u001b[0m \u001b[31m54.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading huggingface_hub-0.26.0-py3-none-any.whl (447 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m447.4/447.4 kB\u001b[0m \u001b[31m31.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading PyYAML-6.0.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (751 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m751.2/751.2 kB\u001b[0m \u001b[31m37.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading regex-2024.9.11-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (782 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m782.7/782.7 kB\u001b[0m \u001b[31m40.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading safetensors-0.4.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (435 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m435.0/435.0 kB\u001b[0m \u001b[31m30.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading tokenizers-0.20.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.0/3.0 MB\u001b[0m \u001b[31m52.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: safetensors, regex, pyyaml, huggingface-hub, tokenizers, transformers\n",
      "Successfully installed huggingface-hub-0.26.0 pyyaml-6.0.2 regex-2024.9.11 safetensors-0.4.5 tokenizers-0.20.1 transformers-4.45.2\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting torch\n",
      "  Downloading torch-2.5.0-cp310-cp310-manylinux1_x86_64.whl.metadata (28 kB)\n",
      "Collecting filelock (from torch)\n",
      "  Downloading filelock-3.16.1-py3-none-any.whl.metadata (2.9 kB)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /home1/fdesousa/.local/lib/python3.10/site-packages (from torch) (4.12.2)\n",
      "Collecting networkx (from torch)\n",
      "  Downloading networkx-3.4.1-py3-none-any.whl.metadata (6.3 kB)\n",
      "Collecting jinja2 (from torch)\n",
      "  Downloading jinja2-3.1.4-py3-none-any.whl.metadata (2.6 kB)\n",
      "Collecting fsspec (from torch)\n",
      "  Downloading fsspec-2024.9.0-py3-none-any.whl.metadata (11 kB)\n",
      "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch)\n",
      "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch)\n",
      "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch)\n",
      "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch)\n",
      "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch)\n",
      "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch)\n",
      "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-curand-cu12==10.3.5.147 (from torch)\n",
      "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch)\n",
      "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch)\n",
      "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-nccl-cu12==2.21.5 (from torch)\n",
      "  Downloading nvidia_nccl_cu12-2.21.5-py3-none-manylinux2014_x86_64.whl.metadata (1.8 kB)\n",
      "Collecting nvidia-nvtx-cu12==12.4.127 (from torch)\n",
      "  Downloading nvidia_nvtx_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.7 kB)\n",
      "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch)\n",
      "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting triton==3.1.0 (from torch)\n",
      "  Downloading triton-3.1.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.3 kB)\n",
      "Collecting sympy==1.13.1 (from torch)\n",
      "  Downloading sympy-1.13.1-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting mpmath<1.4,>=1.1.0 (from sympy==1.13.1->torch)\n",
      "  Downloading mpmath-1.3.0-py3-none-any.whl.metadata (8.6 kB)\n",
      "Collecting MarkupSafe>=2.0 (from jinja2->torch)\n",
      "  Downloading MarkupSafe-3.0.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.0 kB)\n",
      "Downloading torch-2.5.0-cp310-cp310-manylinux1_x86_64.whl (906.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m906.4/906.4 MB\u001b[0m \u001b[31m1.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m46.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m31.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m40.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m16.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_nccl_cu12-2.21.5-py3-none-manylinux2014_x86_64.whl (188.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m188.7/188.7 MB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m34.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_nvtx_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (99 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.1/99.1 kB\u001b[0m \u001b[31m10.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading sympy-1.13.1-py3-none-any.whl (6.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.2/6.2 MB\u001b[0m \u001b[31m53.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading triton-3.1.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (209.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m209.5/209.5 MB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading filelock-3.16.1-py3-none-any.whl (16 kB)\n",
      "Downloading fsspec-2024.9.0-py3-none-any.whl (179 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m179.3/179.3 kB\u001b[0m \u001b[31m15.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading jinja2-3.1.4-py3-none-any.whl (133 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m133.3/133.3 kB\u001b[0m \u001b[31m13.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading networkx-3.4.1-py3-none-any.whl (1.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m45.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hDownloading MarkupSafe-3.0.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (20 kB)\n",
      "Downloading mpmath-1.3.0-py3-none-any.whl (536 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m536.2/536.2 kB\u001b[0m \u001b[31m32.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: mpmath, sympy, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, networkx, MarkupSafe, fsspec, filelock, triton, nvidia-cusparse-cu12, nvidia-cudnn-cu12, jinja2, nvidia-cusolver-cu12, torch\n",
      "Successfully installed MarkupSafe-3.0.2 filelock-3.16.1 fsspec-2024.9.0 jinja2-3.1.4 mpmath-1.3.0 networkx-3.4.1 nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nccl-cu12-2.21.5 nvidia-nvjitlink-cu12-12.4.127 nvidia-nvtx-cu12-12.4.127 sympy-1.13.1 torch-2.5.0 triton-3.1.0\n"
     ]
    }
   ],
   "source": [
    "!pip install torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting scikit-learn\n",
      "  Downloading scikit_learn-1.5.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (13 kB)\n",
      "Requirement already satisfied: numpy>=1.19.5 in /home1/fdesousa/.local/lib/python3.10/site-packages (from scikit-learn) (2.1.2)\n",
      "Collecting scipy>=1.6.0 (from scikit-learn)\n",
      "  Downloading scipy-1.14.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (60 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.8/60.8 kB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting joblib>=1.2.0 (from scikit-learn)\n",
      "  Downloading joblib-1.4.2-py3-none-any.whl.metadata (5.4 kB)\n",
      "Collecting threadpoolctl>=3.1.0 (from scikit-learn)\n",
      "  Downloading threadpoolctl-3.5.0-py3-none-any.whl.metadata (13 kB)\n",
      "Downloading scikit_learn-1.5.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (13.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.3/13.3 MB\u001b[0m \u001b[31m47.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading joblib-1.4.2-py3-none-any.whl (301 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m301.8/301.8 kB\u001b[0m \u001b[31m22.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading scipy-1.14.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (41.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.2/41.2 MB\u001b[0m \u001b[31m21.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading threadpoolctl-3.5.0-py3-none-any.whl (18 kB)\n",
      "Installing collected packages: threadpoolctl, scipy, joblib, scikit-learn\n",
      "Successfully installed joblib-1.4.2 scikit-learn-1.5.2 scipy-1.14.1 threadpoolctl-3.5.0\n"
     ]
    }
   ],
   "source": [
    "!pip install scikit-learn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-19T03:42:21.162100Z",
     "start_time": "2024-10-19T03:42:20.417031Z"
    },
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-19T04:07:50.088239Z",
     "start_time": "2024-10-19T04:07:47.978523Z"
    }
   },
   "outputs": [],
   "source": [
    "#load in all the data sets\n",
    "\n",
    "# kaggle_train = pd.read_csv('datasets/kaggle-data/train_essays.csv') \n",
    "# kaggle_prompts = pd.read_csv('datasets/kaggle-data/train_prompts.csv')\n",
    "\n",
    "#The DAIGT_concatenated dataset can be downloaded from: https://www.kaggle.com/datasets/dsluciano/daigt-one-place-all-data?select=concatenated.csv\n",
    "daigt_external_data = pd.read_csv('DAIGT_concatenated.csv')\n",
    "\n",
    "# test_data_fixed = pd.read_csv('datasets/external-data/test_preprocessed_fixed.csv')\n",
    "# # train_essays_v1 = pd.read_csv('datasets/external-data/train_essays_RDizzl3_seven_v1.csv')\n",
    "# # train_data_fixed = pd.read_csv('datasets/external-data/train_preprocessed_fixed.csv')\n",
    "# # train_v2_raw = pd.read_csv('datasets/external-data/train_v2_drcat_02_raw.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-19T04:13:19.009417Z",
     "start_time": "2024-10-19T04:13:18.999581Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "DAIGT External Data - 'generated' column counts:\n",
      "generated\n",
      "0    29907\n",
      "1    24784\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Count 1s and 0s in 'generated' column for daigt_external_data\n",
    "daigt_external_counts = daigt_external_data['generated'].value_counts()\n",
    "\n",
    "print(\"\\nDAIGT External Data - 'generated' column counts:\")\n",
    "print(daigt_external_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the above output (Kaggle Train - 'generated' column counts:\n",
    "0    1375\n",
    "1       3\n",
    "DAIGT External Data - 'generated' column counts:\n",
    "0    29907\n",
    "1    24784), \n",
    "we disregard the Kaggle training set and work with the compiled externel DAIGT data set instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-19T04:15:59.179805Z",
     "start_time": "2024-10-19T04:15:59.166286Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>prompt_id</th>\n",
       "      <th>text</th>\n",
       "      <th>generated</th>\n",
       "      <th>model</th>\n",
       "      <th>kaggle_repo</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>d429f032</td>\n",
       "      <td>0</td>\n",
       "      <td>Advantages of Limiting Car Usage \\n\\nLimiting ...</td>\n",
       "      <td>1</td>\n",
       "      <td>gpt-3.5-turbo</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1ce279be</td>\n",
       "      <td>0</td>\n",
       "      <td>Advantages of Limiting Car Usage\\n\\nLimiting c...</td>\n",
       "      <td>1</td>\n",
       "      <td>gpt-3.5-turbo</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>c9595213</td>\n",
       "      <td>0</td>\n",
       "      <td>Limiting car usage has numerous advantages tha...</td>\n",
       "      <td>1</td>\n",
       "      <td>gpt-3.5-turbo</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>f2266d87</td>\n",
       "      <td>0</td>\n",
       "      <td>The passages provided discuss the advantages o...</td>\n",
       "      <td>1</td>\n",
       "      <td>gpt-3.5-turbo</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>eeace4bd</td>\n",
       "      <td>0</td>\n",
       "      <td>Title: The Advantages of Limiting Car Usage\\n\\...</td>\n",
       "      <td>1</td>\n",
       "      <td>gpt-3.5-turbo</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         id prompt_id                                               text  \\\n",
       "0  d429f032         0  Advantages of Limiting Car Usage \\n\\nLimiting ...   \n",
       "1  1ce279be         0  Advantages of Limiting Car Usage\\n\\nLimiting c...   \n",
       "2  c9595213         0  Limiting car usage has numerous advantages tha...   \n",
       "3  f2266d87         0  The passages provided discuss the advantages o...   \n",
       "4  eeace4bd         0  Title: The Advantages of Limiting Car Usage\\n\\...   \n",
       "\n",
       "   generated          model  kaggle_repo  \n",
       "0          1  gpt-3.5-turbo            1  \n",
       "1          1  gpt-3.5-turbo            1  \n",
       "2          1  gpt-3.5-turbo            1  \n",
       "3          1  gpt-3.5-turbo            1  \n",
       "4          1  gpt-3.5-turbo            1  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "daigt_external_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Output next word probability vector using AutoModel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sources: [https://huggingface.co/transformers/v3.0.2/model_doc/auto.html](https://huggingface.co/transformers/v3.0.2/model_doc/auto.html)\n",
    "[https://stackoverflow.com/questions/76397904/generate-the-probabilities-of-all-the-next-possible-word-for-a-given-text](https://stackoverflow.com/questions/76397904/generate-the-probabilities-of-all-the-next-possible-word-for-a-given-text)\n",
    "[https://www.kaggle.com/code/funtowiczmo/hugging-face-transformers-get-started](https://www.kaggle.com/code/funtowiczmo/hugging-face-transformers-get-started)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-19T04:34:01.828018Z",
     "start_time": "2024-10-19T04:33:45.295504Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7f0e5bc3f710>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Ensure reproducibility\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-19T04:34:07.135085Z",
     "start_time": "2024-10-19T04:34:07.072872Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training samples: 43752\n",
      "Testing samples: 10939\n"
     ]
    }
   ],
   "source": [
    "# Split into training and testing sets (80% train, 20% test)\n",
    "train_data, test_data = train_test_split(\n",
    "    daigt_external_data, test_size=0.2, random_state=42, stratify=daigt_external_data['generated']\n",
    ")\n",
    "\n",
    "print(f\"Training samples: {len(train_data)}\")\n",
    "print(f\"Testing samples: {len(test_data)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training samples: 2000\n",
      "Testing samples: 400\n"
     ]
    }
   ],
   "source": [
    "#The size of the data above leads to long compute times in processing the data (into vectors of probabilites) and to train the neural network). Let's therefore use a smaller dataset to begin with\n",
    "small_data = daigt_external_data.sample(n=2400, random_state=42)\n",
    "train_data, test_data = train_test_split(\n",
    "    small_data, test_size=400, random_state=42, stratify=small_data['generated']\n",
    ")\n",
    "\n",
    "print(f\"Training samples: {len(train_data)}\")\n",
    "print(f\"Testing samples: {len(test_data)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-19T04:37:31.838870Z",
     "start_time": "2024-10-19T04:37:31.825351Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM , AutoTokenizer\n",
    "#Define language model class\n",
    "class LMHeadModel:\n",
    "\n",
    "    def __init__(self, model_name):\n",
    "        # Initialize the model and the tokenizer.\n",
    "        self.model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    \n",
    "    def get_predictions(self, sentence):\n",
    "        # Encode the sentence using the tokenizer and return the model predictions.\n",
    "        inputs = self.tokenizer.encode(sentence, return_tensors=\"pt\")\n",
    "        with torch.no_grad():\n",
    "            outputs = self.model(inputs)\n",
    "            predictions = outputs[0]\n",
    "        return predictions\n",
    "    \n",
    "    def get_next_word_probabilities(self, sentence, top_k=500):\n",
    "\n",
    "        # Get the model predictions for the sentence.\n",
    "        predictions = self.get_predictions(sentence)\n",
    "        \n",
    "        # Get the next token candidates.\n",
    "        next_token_candidates_tensor = predictions[0, -1, :]\n",
    "\n",
    "        # Get the top k next token candidates.\n",
    "        topk_candidates_indexes = torch.topk(\n",
    "            next_token_candidates_tensor, top_k).indices.tolist()\n",
    "\n",
    "        # Get the token probabilities for all candidates.\n",
    "        all_candidates_probabilities = torch.nn.functional.softmax(\n",
    "            next_token_candidates_tensor, dim=-1)\n",
    "        \n",
    "        # Filter the token probabilities for the top k candidates.\n",
    "        topk_candidates_probabilities = \\\n",
    "            all_candidates_probabilities[topk_candidates_indexes].tolist()\n",
    "\n",
    "        # Decode the top k candidates back to words.\n",
    "        topk_candidates_tokens = \\\n",
    "            [self.tokenizer.decode([idx]).strip() for idx in topk_candidates_indexes]\n",
    "\n",
    "        # Return the top k candidates and their probabilities.\n",
    "        return list(zip(topk_candidates_tokens, topk_candidates_probabilities))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-19T05:12:22.728076Z",
     "start_time": "2024-10-19T05:11:35.755837Z"
    }
   },
   "outputs": [],
   "source": [
    "# Initialize the language model (e.g., GPT-2)\n",
    "#list of models to try:     \n",
    "model_name = 'gpt2'\n",
    "#model_name = 'meta-llama/Llama-3.2-1B', \n",
    "# model_name = 'mistralai/Mistral-7B-v0.1' \n",
    "#model_name = 'EleutherAI/gpt-neo-125M'\n",
    "# model_name = 'distilgpt2'\n",
    "# model_name = 'tiiuae/falcon-7b'\n",
    "lm_model = LMHeadModel(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-19T05:15:40.871950Z",
     "start_time": "2024-10-19T05:15:40.860156Z"
    }
   },
   "outputs": [],
   "source": [
    "def process_text(text, lm_model, num_splits=50):\n",
    "    \"\"\"\n",
    "    Process a single text to generate a vector of true next word probabilities.\n",
    "    \"\"\"\n",
    "    # Split the text into words\n",
    "    words = text.split()\n",
    "    probabilities = []\n",
    "\n",
    "    # Ensure there are enough words to sample\n",
    "    if len(words) < 2:\n",
    "        return [0.0] * num_splits  # Return zeros if not enough words\n",
    "\n",
    "    # Generate 50 split points\n",
    "    for _ in range(num_splits):\n",
    "        # Randomly select a word index (excluding the last word)\n",
    "        split_idx = np.random.randint(1, len(words))\n",
    "        context_words = words[:split_idx]\n",
    "        true_next_word = words[split_idx]\n",
    "\n",
    "        # Reconstruct the context sentence\n",
    "        context_sentence = ' '.join(context_words)\n",
    "\n",
    "        # Get the next word probabilities\n",
    "        try:\n",
    "            next_word_probs = lm_model.get_next_word_probabilities(context_sentence, top_k=500)\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing context: {e}\")\n",
    "            probabilities.append(0.0)\n",
    "            continue\n",
    "\n",
    "        # Find the probability of the true next word\n",
    "        true_word_prob = 0.0\n",
    "        for word, prob in next_word_probs:\n",
    "            if word == true_next_word:\n",
    "                true_word_prob = prob\n",
    "                break  # Stop searching once found\n",
    "\n",
    "        probabilities.append(true_word_prob)\n",
    "\n",
    "    return probabilities\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-19T05:16:30.943607Z",
     "start_time": "2024-10-19T05:15:44.940458Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing training data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|▋         | 131/2000 [03:19<43:49,  1.41s/it] Token indices sequence length is longer than the specified maximum sequence length for this model (1076 > 1024). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing context: index out of range in self\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 17%|█▋        | 347/2000 [08:49<50:00,  1.82s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing context: index out of range in self\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 28%|██▊       | 551/2000 [13:46<30:32,  1.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing context: index out of range in self\n",
      "Error processing context: index out of range in self\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 28%|██▊       | 559/2000 [14:01<47:16,  1.97s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing context: index out of range in self\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 32%|███▏      | 643/2000 [16:11<42:23,  1.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing context: index out of range in self\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 36%|███▌      | 714/2000 [17:56<36:23,  1.70s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing context: index out of range in self\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 39%|███▊      | 771/2000 [19:22<32:33,  1.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing context: index out of range in self\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 45%|████▌     | 902/2000 [22:43<26:53,  1.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing context: index out of range in self\n",
      "Error processing context: index out of range in self\n",
      "Error processing context: index out of range in self\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 46%|████▋     | 926/2000 [23:21<30:21,  1.70s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing context: index out of range in self\n",
      "Error processing context: index out of range in self\n",
      "Error processing context: index out of range in self\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 46%|████▋     | 927/2000 [23:22<29:22,  1.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing context: index out of range in self\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 64%|██████▍   | 1284/2000 [32:17<17:58,  1.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing context: index out of range in self\n",
      "Error processing context: index out of range in self\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 64%|██████▍   | 1285/2000 [32:18<17:48,  1.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing context: index out of range in self\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 69%|██████▉   | 1389/2000 [34:50<13:38,  1.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing context: index out of range in self\n",
      "Error processing context: index out of range in self\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 72%|███████▏  | 1442/2000 [36:15<16:17,  1.75s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing context: index out of range in self\n",
      "Error processing context: index out of range in self\n",
      "Error processing context: index out of range in self\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 72%|███████▏  | 1443/2000 [36:16<14:47,  1.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing context: index out of range in self\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 77%|███████▋  | 1541/2000 [38:42<12:39,  1.65s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing context: index out of range in self\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 83%|████████▎ | 1654/2000 [41:30<08:00,  1.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing context: index out of range in self\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 96%|█████████▌| 1910/2000 [47:56<02:36,  1.74s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing context: index out of range in self\n",
      "Error processing context: index out of range in self\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 98%|█████████▊| 1963/2000 [49:19<00:48,  1.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing context: index out of range in self\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2000/2000 [50:18<00:00,  1.51s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing testing data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|██▉       | 118/400 [14:43<40:17,  8.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing context: index out of range in self\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 36%|███▌      | 143/400 [17:45<26:08,  6.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing context: index out of range in self\n",
      "Error processing context: index out of range in self\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 62%|██████▏   | 246/400 [31:11<18:37,  7.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing context: index out of range in self\n",
      "Error processing context: index out of range in self\n",
      "Error processing context: index out of range in self\n",
      "Error processing context: index out of range in self\n",
      "Error processing context: index out of range in self\n",
      "Error processing context: index out of range in self\n",
      "Error processing context: index out of range in self\n",
      "Error processing context: index out of range in self\n",
      "Error processing context: index out of range in self\n",
      "Error processing context: index out of range in self\n",
      "Error processing context: index out of range in self\n",
      "Error processing context: index out of range in self\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 62%|██████▏   | 247/400 [31:21<20:39,  8.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing context: index out of range in self\n",
      "Error processing context: index out of range in self\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 66%|██████▋   | 266/400 [33:51<16:44,  7.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing context: index out of range in self\n",
      "Error processing context: index out of range in self\n",
      "Error processing context: index out of range in self\n",
      "Error processing context: index out of range in self\n",
      "Error processing context: index out of range in self\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 72%|███████▏  | 286/400 [36:18<13:08,  6.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing context: index out of range in self\n",
      "Error processing context: index out of range in self\n",
      "Error processing context: index out of range in self\n",
      "Error processing context: index out of range in self\n",
      "Error processing context: index out of range in self\n",
      "Error processing context: index out of range in self\n",
      "Error processing context: index out of range in self\n",
      "Error processing context: index out of range in self\n",
      "Error processing context: index out of range in self\n",
      "Error processing context: index out of range in self\n",
      "Error processing context: index out of range in self\n",
      "Error processing context: index out of range in self\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 72%|███████▏  | 287/400 [36:28<14:54,  7.92s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing context: index out of range in self\n",
      "Error processing context: index out of range in self\n",
      "Error processing context: index out of range in self\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 400/400 [50:13<00:00,  7.53s/it]\n"
     ]
    }
   ],
   "source": [
    "# Prepare lists to store the results\n",
    "X_train = []\n",
    "y_train = []\n",
    "\n",
    "# Process training data\n",
    "print(\"Processing training data...\")\n",
    "for idx, row in tqdm(train_data.iterrows(), total=len(train_data)):\n",
    "    text = row['text']\n",
    "    label = row['generated']\n",
    "    probs_vector = process_text(text, lm_model, num_splits=10)\n",
    "    X_train.append(probs_vector)\n",
    "    y_train.append(label)\n",
    "\n",
    "# Convert lists to tensors or arrays\n",
    "X_train = np.array(X_train)\n",
    "y_train = np.array(y_train)\n",
    "# Save the numpy arrays to .npy files\n",
    "np.save('X_train.npy', X_train)\n",
    "np.save('y_train.npy', y_train)\n",
    "\n",
    "# Similarly process the test data\n",
    "X_test = []\n",
    "y_test = []\n",
    "\n",
    "print(\"Processing testing data...\")\n",
    "for idx, row in tqdm(test_data.iterrows(), total=len(test_data)):\n",
    "    text = row['text']\n",
    "    label = row['generated']\n",
    "    probs_vector = process_text(text, lm_model, num_splits=10)\n",
    "    X_test.append(probs_vector)\n",
    "    y_test.append(label)\n",
    "\n",
    "X_test = np.array(X_test)\n",
    "y_test = np.array(y_test)\n",
    "# Save the numpy arrays to .npy files\n",
    "np.save('X_test.npy', X_test)\n",
    "np.save('y_test.npy', y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Prepare to feed into neural net\n",
    "#Replace zeros with a small value to avoid issues in log transformation\n",
    "epsilon = 1e-10\n",
    "X_train = np.where(X_train == 0, epsilon, X_train)\n",
    "X_test = np.where(X_test == 0, epsilon, X_test)\n",
    "\n",
    "# Optionally, apply log transformation\n",
    "X_train = np.log(X_train)\n",
    "X_test = np.log(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "# Convert data to PyTorch tensors\n",
    "X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y_train, dtype=torch.long)\n",
    "\n",
    "X_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
    "y_test_tensor = torch.tensor(y_test, dtype=torch.long)\n",
    "\n",
    "# Create datasets and data loaders\n",
    "batch_size = 32\n",
    "\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForwardNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size=64):\n",
    "        super(FeedForwardNN, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(hidden_size, 2)  # Binary classification\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.fc1(x)\n",
    "        out = self.relu(out)\n",
    "        out = self.fc2(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = X_train.shape[1]\n",
    "model = FeedForwardNN(input_size)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Loss: 0.6638\n",
      "Epoch [2/10], Loss: 0.6539\n",
      "Epoch [3/10], Loss: 0.6488\n",
      "Epoch [4/10], Loss: 0.6374\n",
      "Epoch [5/10], Loss: 0.6392\n",
      "Epoch [6/10], Loss: 0.6292\n",
      "Epoch [7/10], Loss: 0.6290\n",
      "Epoch [8/10], Loss: 0.6193\n",
      "Epoch [9/10], Loss: 0.6191\n",
      "Epoch [10/10], Loss: 0.6140\n"
     ]
    }
   ],
   "source": [
    "#Train the network\n",
    "num_epochs = 10\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "\n",
    "    for X_batch, y_batch in train_loader:\n",
    "        # Zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(X_batch)\n",
    "        loss = criterion(outputs, y_batch)\n",
    "\n",
    "        # Backward pass and optimization\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    # Calculate average loss over the epoch\n",
    "    epoch_loss = running_loss / len(train_loader)\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {epoch_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape: (2000, 10)\n",
      "X_test shape: (400, 50)\n"
     ]
    }
   ],
   "source": [
    "print(\"X_train shape:\", X_train.shape)\n",
    "print(\"X_test shape:\", X_test.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Randomly select 10 indices from 0 to 49 without replacement\n",
    "selected_indices = np.random.choice(50, size=10, replace=False)\n",
    "\n",
    "# Select the probabilities at the chosen indices\n",
    "X_test_adjusted = X_test[:, selected_indices]\n",
    "\n",
    "# Proceed with evaluation using X_test_adjusted\n",
    "X_test_tensor = torch.tensor(X_test_adjusted, dtype=torch.float32)\n",
    "y_test_tensor = torch.tensor(y_test, dtype=torch.long)\n",
    "\n",
    "# Create a new DataLoader for the adjusted test data\n",
    "test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Evaluate model\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "model.eval()\n",
    "all_preds = []\n",
    "all_labels = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for X_batch, y_batch in test_loader:\n",
    "        outputs = model(X_batch)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        all_preds.extend(predicted.cpu().numpy())\n",
    "        all_labels.extend(y_batch.cpu().numpy())\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(all_labels, all_preds)\n",
    "print(f\"Test Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "# Print classification report\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(all_labels, all_preds, digits=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using only gpt-2 and 10 split points per text, we get the following results: Test Accuracy: 0.6225\n",
    "Classification Report:\n",
    "              precision    recall  f1-score   support\n",
    "\n",
    "           0     0.7027    0.5752    0.6326       226\n",
    "           1     0.5535    0.6839    0.6118       174\n",
    "\n",
    "    accuracy                         0.6225       400\n",
    "   macro avg     0.6281    0.6296    0.6222       400\n",
    "weighted avg     0.6378    0.6225    0.6236       400"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sanat's code is below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-03T00:21:53.818978Z",
     "iopub.status.busy": "2024-10-03T00:21:53.818537Z",
     "iopub.status.idle": "2024-10-03T00:21:53.825315Z",
     "shell.execute_reply": "2024-10-03T00:21:53.824160Z",
     "shell.execute_reply.started": "2024-10-03T00:21:53.818950Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cars. Cars have been around since they became famous in the 1900s, when Henry Ford created and built the first ModelT. Cars have played a major role in our every day lives since then. But now, people are starting to question if limiting car usage would be a good thing. To me, limiting the use of cars might be a good thing to do.\n",
      "\n",
      "In like matter of this, article, \"In German Suburb, Life Goes On Without Cars,\" by Elizabeth Rosenthal states, how automobiles are the linchpin of suburbs, where middle class families from either Shanghai or Chicago tend to make their homes. Experts say how this is a huge impediment to current efforts to reduce greenhouse gas emissions from tailpipe. Passenger cars are responsible for 12 percent of greenhouse gas emissions in Europe...and up to 50 percent in some carintensive areas in the United States. Cars are the main reason for the greenhouse gas emissions because of a lot of people driving them around all the time getting where they need to go. Article, \"Paris bans driving due to smog,\" by Robert Duffer says, how Paris, after days of nearrecord pollution, enforced a partial driving ban to clear the air of the global city. It also says, how on Monday, motorist with evennumbered license plates were ordered to leave their cars at home or be fined a 22euro fine 31. The same order would be applied to oddnumbered plates the following day. Cars are the reason for polluting entire cities like Paris. This shows how bad cars can be because, of all the pollution that they can cause to an entire city.\n",
      "\n",
      "Likewise, in the article, \"Carfree day is spinning into a big hit in Bogota,\" by Andrew Selsky says, how programs that's set to spread to other countries, millions of Columbians hiked, biked, skated, or took the bus to work during a carfree day, leaving streets of this capital city eerily devoid of traffic jams. It was the third straight year cars have been banned with only buses and taxis permitted for the Day Without Cars in the capital city of 7 million. People like the idea of having carfree days because, it allows them to lesson the pollution that cars put out of their exhaust from people driving all the time. The article also tells how parks and sports centers have bustled throughout the city uneven, pitted sidewalks have been replaced by broad, smooth sidewalks rushhour restrictions have dramatically cut traffic and new restaurants and upscale shopping districts have cropped up. Having no cars has been good for the country of Columbia because, it has aloud them to repair things that have needed repairs for a long time, traffic jams have gone down, and restaurants and shopping districts have popped up, all due to the fact of having less cars around.\n",
      "\n",
      "In conclusion, the use of less cars and having carfree days, have had a big impact on the environment of cities because, it is cutting down the air pollution that the cars have majorly polluted, it has aloud countries like Columbia to repair sidewalks, and cut down traffic jams. Limiting the use of cars would be a good thing for America. So we should limit the use of cars by maybe riding a bike, or maybe walking somewhere that isn't that far from you and doesn't need the use of a car to get you there. To me, limiting the use of cars might be a good thing to do.\n"
     ]
    }
   ],
   "source": [
    "print(kaggle_train['text'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-03T00:21:53.826933Z",
     "iopub.status.busy": "2024-10-03T00:21:53.826617Z",
     "iopub.status.idle": "2024-10-03T00:21:53.848385Z",
     "shell.execute_reply": "2024-10-03T00:21:53.847121Z",
     "shell.execute_reply.started": "2024-10-03T00:21:53.826905Z"
    }
   },
   "outputs": [],
   "source": [
    "sentence1, sentence2, sentence3 = kaggle_train['text'][0][:59], kaggle_train['text'][0][:350], kaggle_train['text'][0][:749]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-03T00:21:53.850571Z",
     "iopub.status.busy": "2024-10-03T00:21:53.850184Z",
     "iopub.status.idle": "2024-10-03T00:22:13.779596Z",
     "shell.execute_reply": "2024-10-03T00:22:13.778339Z",
     "shell.execute_reply.started": "2024-10-03T00:21:53.850542Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "28cd7f407241446e9bdcbf4b214dde3c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5eaceba024394a4f888b64c5522b7dbe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading model.safetensors:   0%|          | 0.00/436M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "If you want to use `BertLMHeadModel` as a standalone, add `is_decoder=True.`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "80327b96dd4e41a69aeb4001a798e4b6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading tokenizer_config.json:   0%|          | 0.00/49.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7afa566a2aab46d4ba0e60eec979ec76",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading vocab.txt:   0%|          | 0.00/213k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d9264551474e4671b1cffaccc8ded2df",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading tokenizer.json:   0%|          | 0.00/436k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[('.', 0.9998961687088013),\n",
       " ('!', 1.0118837053596508e-05),\n",
       " (',', 9.749163837113883e-06),\n",
       " ('?', 8.029621312743984e-06),\n",
       " (';', 7.971573722898029e-06),\n",
       " ('and', 7.239583283080719e-06),\n",
       " ('\"', 6.454437880165642e-06),\n",
       " ('...', 4.2128358472837135e-06),\n",
       " ('it', 4.098200861335499e-06),\n",
       " ('It', 2.7247053822065936e-06),\n",
       " ('This', 1.9060513523072586e-06),\n",
       " ('that', 1.905687781800225e-06),\n",
       " (':', 1.5619426676494186e-06),\n",
       " ('of', 1.5250606111294474e-06),\n",
       " ('this', 1.4305770719147404e-06),\n",
       " ('the', 1.2741787713821395e-06),\n",
       " ('people', 1.200709561999247e-06),\n",
       " ('who', 9.789341675059404e-07),\n",
       " ('-', 8.085035005933605e-07),\n",
       " ('in', 7.963218422446516e-07),\n",
       " ('##s', 7.858031949581346e-07),\n",
       " ('to', 7.434784947690787e-07),\n",
       " ('Auto', 7.291108659046586e-07),\n",
       " ('have', 6.775054544050363e-07),\n",
       " ('##ly', 6.183860250530415e-07),\n",
       " ('which', 6.102171141719737e-07),\n",
       " ('what', 5.308630193212593e-07),\n",
       " ('an', 5.250657295619021e-07),\n",
       " ('a', 5.180836524232291e-07),\n",
       " ('their', 4.950094307787367e-07),\n",
       " ('He', 4.888082116849546e-07),\n",
       " (\"'\", 4.681031384734524e-07),\n",
       " ('he', 4.4954316535950056e-07),\n",
       " ('be', 4.4780378516406927e-07),\n",
       " ('/', 4.2096570496141794e-07),\n",
       " ('Cars', 4.1760287672332197e-07),\n",
       " ('That', 4.1226007851946633e-07),\n",
       " ('they', 3.852777581414557e-07),\n",
       " ('t', 3.72708882423467e-07),\n",
       " ('Museum', 3.6711048778670374e-07),\n",
       " ('The', 3.613403976032714e-07),\n",
       " ('Character', 3.610496719375078e-07),\n",
       " ('is', 3.609484338085167e-07),\n",
       " ('as', 3.383490252417687e-07),\n",
       " ('there', 3.3309925129287876e-07),\n",
       " ('them', 2.8712457833535154e-07),\n",
       " ('s', 2.835657539890235e-07),\n",
       " ('An', 2.741863340816053e-07),\n",
       " ('Their', 2.663635427779809e-07),\n",
       " ('I', 2.5838465944616473e-07),\n",
       " (')', 2.527467586332932e-07),\n",
       " ('She', 2.462140002990054e-07),\n",
       " ('They', 2.4487968630637624e-07),\n",
       " ('are', 2.1541092110055615e-07),\n",
       " ('(', 2.125304803257677e-07),\n",
       " ('she', 2.115694996973616e-07),\n",
       " ('TV', 2.067903324132203e-07),\n",
       " ('1', 1.9760619807129842e-07),\n",
       " ('for', 1.9462498812572449e-07),\n",
       " ('Art', 1.9165257469921926e-07),\n",
       " ('until', 1.8326794304357463e-07),\n",
       " ('one', 1.6270089986392122e-07),\n",
       " ('was', 1.5810516629244376e-07),\n",
       " ('People', 1.5251065121901775e-07),\n",
       " ('being', 1.472596409257676e-07),\n",
       " ('however', 1.4609943832510908e-07),\n",
       " ('when', 1.4229050293579348e-07),\n",
       " ('America', 1.4060032071938622e-07),\n",
       " ('Co', 1.3209084670506854e-07),\n",
       " ('There', 1.311495623212977e-07),\n",
       " ('from', 1.2406466964876017e-07),\n",
       " ('time', 1.2266291093965265e-07),\n",
       " ('these', 1.223009036266376e-07),\n",
       " ('er', 1.1958009338286502e-07),\n",
       " ('were', 1.1797715160355438e-07),\n",
       " ('B', 1.1785221687432568e-07),\n",
       " ('One', 1.1761137841403979e-07),\n",
       " ('because', 1.1735010474467344e-07),\n",
       " ('M', 1.164706446843411e-07),\n",
       " ('here', 1.1611363959218579e-07),\n",
       " ('International', 1.157586098088359e-07),\n",
       " ('so', 1.1009271361217543e-07),\n",
       " ('A', 1.0899496061256286e-07),\n",
       " ('am', 1.0885555212780673e-07),\n",
       " ('on', 1.0353483759217852e-07),\n",
       " ('Motor', 1.0011368800633136e-07),\n",
       " ('T', 9.77332064167058e-08),\n",
       " ('R', 9.647213516927877e-08),\n",
       " ('C', 8.359435810234572e-08),\n",
       " ('where', 8.180116850553532e-08),\n",
       " ('something', 8.007482676930522e-08),\n",
       " ('more', 7.88293235132187e-08),\n",
       " ('These', 7.446731586924216e-08),\n",
       " ('culture', 7.072377172789857e-08),\n",
       " ('too', 6.857673184867963e-08),\n",
       " ('i', 6.817244724288685e-08),\n",
       " ('Show', 6.811292507791222e-08),\n",
       " ('you', 6.723750800574635e-08),\n",
       " ('but', 6.637892369099063e-08),\n",
       " ('Z', 6.466414959049871e-08),\n",
       " ('V', 6.433494093016634e-08),\n",
       " ('your', 6.298826349393494e-08),\n",
       " ('its', 6.289954512794793e-08),\n",
       " ('her', 6.280040310002732e-08),\n",
       " ('by', 6.233725002857682e-08),\n",
       " ('Anyway', 6.136529862033058e-08),\n",
       " ('has', 6.077937797499544e-08),\n",
       " ('if', 5.999238084086755e-08),\n",
       " ('everyone', 5.849199524732285e-08),\n",
       " ('ever', 5.6564179118367974e-08),\n",
       " ('someone', 5.593238228129849e-08),\n",
       " ('his', 5.532969638011309e-08),\n",
       " ('Cinema', 5.509412304149919e-08),\n",
       " ('or', 5.485704335228547e-08),\n",
       " ('S', 5.362543831211042e-08),\n",
       " ('2', 5.119317947333002e-08),\n",
       " ('Company', 4.805083264614041e-08),\n",
       " ('Home', 4.705103506807973e-08),\n",
       " ('AD', 4.6823458887956804e-08),\n",
       " ('today', 4.590800273263085e-08),\n",
       " ('like', 4.544886067492371e-08),\n",
       " ('X', 4.4397474141533166e-08),\n",
       " ('r', 4.380054363650743e-08),\n",
       " ('Victoria', 4.361796257512651e-08),\n",
       " ('P', 4.312542500883865e-08),\n",
       " ('##eal', 4.310059154022383e-08),\n",
       " ('such', 4.250641794101284e-08),\n",
       " ('Here', 4.234941997083297e-08),\n",
       " ('His', 4.196266800704507e-08),\n",
       " ('What', 4.1872247891205916e-08),\n",
       " ('##T', 4.122048480326157e-08),\n",
       " ('Automobile', 4.098224337667489e-08),\n",
       " ('Its', 4.0528739475576003e-08),\n",
       " ('Europe', 3.930720993139403e-08),\n",
       " ('cars', 3.9091805348334674e-08),\n",
       " ('Inc', 3.893851285852179e-08),\n",
       " ('anything', 3.847620178021316e-08),\n",
       " ('had', 3.812780846601527e-08),\n",
       " ('G', 3.7917203599135973e-08),\n",
       " ('at', 3.7675338404596914e-08),\n",
       " ('E', 3.7482170256453173e-08),\n",
       " ('H', 3.720999330880659e-08),\n",
       " ('Japan', 3.5475228088444055e-08),\n",
       " ('although', 3.50810616112085e-08),\n",
       " ('##to', 3.4387639402666537e-08),\n",
       " ('Film', 3.422470840064307e-08),\n",
       " ('company', 3.418204386207435e-08),\n",
       " ('Rosa', 3.412035809446934e-08),\n",
       " ('Mexico', 3.294278272392148e-08),\n",
       " ('In', 3.291615513489887e-08),\n",
       " ('Travel', 3.2763818325065586e-08),\n",
       " ('characters', 3.1683548229466396e-08),\n",
       " ('ones', 3.155123806664051e-08),\n",
       " ('So', 3.125069980569606e-08),\n",
       " ('while', 3.091219369366627e-08),\n",
       " ('Xi', 3.061505537971243e-08),\n",
       " ('car', 3.049639829555417e-08),\n",
       " ('how', 2.998050518954187e-08),\n",
       " ('everything', 2.9679526392101252e-08),\n",
       " ('Today', 2.9584180438746444e-08),\n",
       " ('To', 2.8626429227074368e-08),\n",
       " ('women', 2.8427720621948538e-08),\n",
       " ('movement', 2.8381940353483515e-08),\n",
       " ('again', 2.813739108376012e-08),\n",
       " ('&', 2.8121188933027952e-08),\n",
       " ('Disco', 2.8097222326550764e-08),\n",
       " ('National', 2.7534859725619754e-08),\n",
       " ('age', 2.751837513415012e-08),\n",
       " ('First', 2.746698513078627e-08),\n",
       " ('Association', 2.7452477624478888e-08),\n",
       " ('Route', 2.7238426625331158e-08),\n",
       " ('industry', 2.6520352136572e-08),\n",
       " ('[UNK]', 2.642592811241684e-08),\n",
       " ('e', 2.640688379074163e-08),\n",
       " ('Oh', 2.61076493757173e-08),\n",
       " ('And', 2.5234081491021243e-08),\n",
       " ('Music', 2.500410900552197e-08),\n",
       " ('though', 2.4910667306698997e-08),\n",
       " ('Industry', 2.4890006500299933e-08),\n",
       " ('many', 2.4039385593255247e-08),\n",
       " ('c', 2.3871697507615863e-08),\n",
       " ('Digital', 2.3863048426164823e-08),\n",
       " ('France', 2.371413465596106e-08),\n",
       " ('television', 2.356349781962308e-08),\n",
       " ('y', 2.3495102752235653e-08),\n",
       " ('1930', 2.34225723261261e-08),\n",
       " ('Fun', 2.3206277788290208e-08),\n",
       " ('Am', 2.3118936098853737e-08),\n",
       " ('automobiles', 2.256728315330747e-08),\n",
       " ('every', 2.226275519490173e-08),\n",
       " ('General', 2.20638618486646e-08),\n",
       " ('order', 2.1943117545220048e-08),\n",
       " ('China', 2.1619039003439866e-08),\n",
       " ('always', 2.136863663793065e-08),\n",
       " ('Kingdom', 2.124193088093307e-08),\n",
       " ('Cartoon', 2.118932762584791e-08),\n",
       " ('##p', 2.0940227329901973e-08),\n",
       " ('##1', 2.0767046748915163e-08),\n",
       " ('##M', 2.055472059225849e-08),\n",
       " ('1910', 2.0329400385321605e-08),\n",
       " ('Everything', 2.030986756551556e-08),\n",
       " ('Her', 2.0295848557339013e-08),\n",
       " ('##li', 2.027909395962979e-08),\n",
       " ('each', 2.0161500913218333e-08),\n",
       " ('Central', 2.0140555889724965e-08),\n",
       " ('ve', 2.0019795599068857e-08),\n",
       " ('Is', 1.9928398486968035e-08),\n",
       " ('Labor', 1.9910125104161125e-08),\n",
       " ('him', 1.9897369085697392e-08),\n",
       " ('love', 1.9688451757815528e-08),\n",
       " ('DC', 1.9442007115344495e-08),\n",
       " ('1900', 1.932606430443684e-08),\n",
       " ('town', 1.927301873649867e-08),\n",
       " ('Many', 1.9143692853162975e-08),\n",
       " ('very', 1.904866309132558e-08),\n",
       " ('##sp', 1.8908323795585602e-08),\n",
       " ('since', 1.848952280170124e-08),\n",
       " ('will', 1.8341786756082e-08),\n",
       " ('Victor', 1.828687423710562e-08),\n",
       " ('0', 1.8234734611155545e-08),\n",
       " ('man', 1.8186488759397434e-08),\n",
       " ('You', 1.8155262182517617e-08),\n",
       " ('Man', 1.812598959816114e-08),\n",
       " ('h', 1.8025627213091866e-08),\n",
       " ('##H', 1.7953297515305167e-08),\n",
       " ('|', 1.758631462678295e-08),\n",
       " ('Corporation', 1.7505595195643764e-08),\n",
       " ('AC', 1.736346888492335e-08),\n",
       " ('Brand', 1.7361415416417003e-08),\n",
       " ('Foreign', 1.7211036151820736e-08),\n",
       " ('Your', 1.71331819842635e-08),\n",
       " ('Do', 1.6911528177843138e-08),\n",
       " ('know', 1.659537929299404e-08),\n",
       " ('m', 1.6416620951531513e-08),\n",
       " ('Luce', 1.6362477595066593e-08),\n",
       " ('Say', 1.6253736134785868e-08),\n",
       " ('Love', 1.6229849464366453e-08),\n",
       " ('then', 1.6083076204154167e-08),\n",
       " ('##e', 1.607059552100054e-08),\n",
       " ('Albert', 1.5930668340047305e-08),\n",
       " ('##I', 1.5884456416870307e-08),\n",
       " ('Hu', 1.585654807456649e-08),\n",
       " ('my', 1.5716025814072054e-08),\n",
       " ('##A', 1.5637684924740824e-08),\n",
       " ('etc', 1.5569115774383135e-08),\n",
       " ('##life', 1.5349387538776682e-08),\n",
       " ('Ever', 1.5244179252249523e-08),\n",
       " ('Germany', 1.5143733378408797e-08),\n",
       " ('really', 1.5094393290837615e-08),\n",
       " ('reason', 1.487864054183774e-08),\n",
       " ('Society', 1.480379641094487e-08),\n",
       " ('Hall', 1.476330524496916e-08),\n",
       " ('##ity', 1.4610216148014388e-08),\n",
       " ('Ah', 1.4517604007835416e-08),\n",
       " ('becoming', 1.4318422891790306e-08),\n",
       " ('1922', 1.429912721562232e-08),\n",
       " ('Ko', 1.4141840587456045e-08),\n",
       " ('Government', 1.4132807812927695e-08),\n",
       " ('Machine', 1.4095092204513548e-08),\n",
       " ('##ia', 1.3967941470127698e-08),\n",
       " ('Women', 1.3958913136491446e-08),\n",
       " ('Day', 1.3858264757971028e-08),\n",
       " ('those', 1.379241876264814e-08),\n",
       " ('think', 1.377393843426944e-08),\n",
       " ('last', 1.3741189519578256e-08),\n",
       " ('Television', 1.3712311286440126e-08),\n",
       " ('##C', 1.3694093858873657e-08),\n",
       " ('##L', 1.3686417332792189e-08),\n",
       " ('World', 1.3522399200383006e-08),\n",
       " ('Hollywood', 1.3504227958094361e-08),\n",
       " ('II', 1.3405298204816063e-08),\n",
       " ('around', 1.3337661641799059e-08),\n",
       " ('become', 1.3180664559797606e-08),\n",
       " ('about', 1.3001661081091243e-08),\n",
       " ('Radio', 1.298659313420103e-08),\n",
       " ('o', 1.2935124082957827e-08),\n",
       " ('##hol', 1.2895290169012696e-08),\n",
       " ('Flight', 1.2837448437608145e-08),\n",
       " ('Video', 1.2825432271768022e-08),\n",
       " ('All', 1.277028971458094e-08),\n",
       " ('##po', 1.2728099463288345e-08),\n",
       " ('Ma', 1.2566023777083046e-08),\n",
       " ('artists', 1.2561685913681231e-08),\n",
       " ('Real', 1.248089187555479e-08),\n",
       " ('Car', 1.2456538911465032e-08),\n",
       " ('anymore', 1.2391278225720725e-08),\n",
       " ('men', 1.2106451841020771e-08),\n",
       " ('Park', 1.1897748564138055e-08),\n",
       " ('##t', 1.1896500673458377e-08),\n",
       " ('anyone', 1.1881081007913963e-08),\n",
       " ('with', 1.180218767160568e-08),\n",
       " ('persons', 1.1716732473132652e-08),\n",
       " ('character', 1.1705942881690135e-08),\n",
       " ('However', 1.1680294065286034e-08),\n",
       " ('Novel', 1.1575568947819193e-08),\n",
       " ('Something', 1.1517207632039117e-08),\n",
       " ('2010', 1.1372849328949997e-08),\n",
       " ('having', 1.1357090379249257e-08),\n",
       " ('Industrial', 1.1345703043730282e-08),\n",
       " ('Main', 1.1335233196518857e-08),\n",
       " ('##ism', 1.1325789195382185e-08),\n",
       " ('machines', 1.1287337287058108e-08),\n",
       " ('David', 1.1284238432551774e-08),\n",
       " ('History', 1.1249081666164784e-08),\n",
       " ('Everyone', 1.1184363657434915e-08),\n",
       " ('Be', 1.1138933331267253e-08),\n",
       " ('all', 1.1130862453967438e-08),\n",
       " ('work', 1.1100946828435099e-08),\n",
       " ('Cha', 1.1078631345640133e-08),\n",
       " ('##co', 1.1062773808134807e-08),\n",
       " ('N', 1.104756996994638e-08),\n",
       " ('Old', 1.090217249810621e-08),\n",
       " ('himself', 1.0850537357498524e-08),\n",
       " ('University', 1.0848468789959043e-08),\n",
       " ('Ava', 1.082315126410549e-08),\n",
       " ('Local', 1.077579536712392e-08),\n",
       " ('artist', 1.0683213425011218e-08),\n",
       " ('Because', 1.0672845718318058e-08),\n",
       " ('My', 1.0635921476875865e-08),\n",
       " ('first', 1.060110665918046e-08),\n",
       " ('whenever', 1.0598276922735295e-08),\n",
       " ('family', 1.0536599148736059e-08),\n",
       " ('Mill', 1.0521277182817812e-08),\n",
       " ('revolution', 1.0501788771932752e-08),\n",
       " ('es', 1.0403541139680783e-08),\n",
       " ('Hyde', 1.0370218461730474e-08),\n",
       " ('Mom', 1.0359226365608265e-08),\n",
       " ('Death', 1.0350990287122386e-08),\n",
       " ('auto', 1.0331916655559326e-08),\n",
       " ('##y', 1.0298234265349038e-08),\n",
       " ('##X', 1.0291734575673672e-08),\n",
       " ('families', 1.0282139584205652e-08),\n",
       " ('Production', 1.0270849948312843e-08),\n",
       " ('been', 1.0252921178732777e-08),\n",
       " ('More', 1.0081534718153762e-08),\n",
       " ('became', 1.0080995593853004e-08),\n",
       " ('children', 1.00027550686832e-08),\n",
       " ('z', 9.9673318487703e-09),\n",
       " ('Rome', 9.860475991274598e-09),\n",
       " ('after', 9.841911285946026e-09),\n",
       " ('##chan', 9.811023993222534e-09),\n",
       " ('Department', 9.808049483694958e-09),\n",
       " ('Consumer', 9.775775744458315e-09),\n",
       " ('Although', 9.58706092291095e-09),\n",
       " ('J', 9.583879467811585e-09),\n",
       " ('Luna', 9.566366365731938e-09),\n",
       " ('Records', 9.5611483175162e-09),\n",
       " ('back', 9.543566825698235e-09),\n",
       " ('Business', 9.51100354029677e-09),\n",
       " ('Pen', 9.463485106664393e-09),\n",
       " ('Mo', 9.422958413551896e-09),\n",
       " ('companies', 9.407532530758544e-09),\n",
       " ('##l', 9.373358977882162e-09),\n",
       " ('quite', 9.2591561084987e-09),\n",
       " ('Life', 9.255554545006817e-09),\n",
       " ('##à', 9.186137184258314e-09),\n",
       " ('into', 9.146973845020057e-09),\n",
       " ('1920', 9.126531530512239e-09),\n",
       " ('much', 9.068852335758493e-09),\n",
       " ('life', 9.043305659872658e-09),\n",
       " ('##r', 9.035202808149734e-09),\n",
       " ('em', 9.014460289336057e-09),\n",
       " ('##G', 9.003634282578332e-09),\n",
       " ('still', 8.946731355763404e-09),\n",
       " ('times', 8.93646490141009e-09),\n",
       " ('Taiwan', 8.934027739826433e-09),\n",
       " ('Canada', 8.919418981179206e-09),\n",
       " ('Of', 8.917088401005913e-09),\n",
       " ('##illa', 8.89565043848961e-09),\n",
       " ('Florence', 8.864097011951344e-09),\n",
       " ('exist', 8.849906585339795e-09),\n",
       " ('home', 8.815168150988484e-09),\n",
       " ('production', 8.814360796804976e-09),\n",
       " ('Culture', 8.812058638341114e-09),\n",
       " ('Inn', 8.788023642125609e-09),\n",
       " ('g', 8.771260162632188e-09),\n",
       " ('do', 8.762130576656091e-09),\n",
       " ('ch', 8.761679382018883e-09),\n",
       " ('But', 8.720598465572493e-09),\n",
       " ('Mansion', 8.70423022547584e-09),\n",
       " ('##uli', 8.694738262704504e-09),\n",
       " ('show', 8.677475626939213e-09),\n",
       " ('government', 8.632491166338241e-09),\n",
       " ('Mu', 8.607188739517824e-09),\n",
       " ('Such', 8.594245315407534e-09),\n",
       " ('state', 8.589689848292892e-09),\n",
       " ('City', 8.580945731750944e-09),\n",
       " ('##n', 8.534182249775313e-09),\n",
       " ('arrival', 8.456598976636087e-09),\n",
       " ('Speed', 8.455792510631e-09),\n",
       " ('##sm', 8.434546394653353e-09),\n",
       " ('school', 8.383191030247872e-09),\n",
       " ('say', 8.358124858887095e-09),\n",
       " ('Ji', 8.32517432769464e-09),\n",
       " ('said', 8.232829529219998e-09),\n",
       " ('Ch', 8.181952892982736e-09),\n",
       " ('##ci', 8.156384012636408e-09),\n",
       " ('Avalon', 8.0647124534039e-09),\n",
       " ('For', 8.02247601683348e-09),\n",
       " ('##R', 8.002379203730925e-09),\n",
       " ('Period', 7.986765027112597e-09),\n",
       " ('Cam', 7.950636593534455e-09),\n",
       " ('1929', 7.940801793893115e-09),\n",
       " ('p', 7.904368715117016e-09),\n",
       " ('ll', 7.866406193102193e-09),\n",
       " ('U', 7.852000827313077e-09),\n",
       " ('Toy', 7.84985942914318e-09),\n",
       " ('CP', 7.81354536627532e-09),\n",
       " ('some', 7.810164071031522e-09),\n",
       " ('##sca', 7.786351119420942e-09),\n",
       " ('1902', 7.729721751559282e-09),\n",
       " ('1940', 7.676783653209895e-09),\n",
       " ('Public', 7.637424026540884e-09),\n",
       " ('Men', 7.578654148687747e-09),\n",
       " ('business', 7.540817748008521e-09),\n",
       " ('me', 7.521326672588202e-09),\n",
       " ('Porsche', 7.49920836540241e-09),\n",
       " ('Every', 7.4288100115893485e-09),\n",
       " ('Go', 7.417751746174872e-09),\n",
       " ('living', 7.3848593906689075e-09),\n",
       " ('existence', 7.342667807108683e-09),\n",
       " ('manufacture', 7.3063031180709e-09),\n",
       " ('vehicles', 7.2708257192743986e-09),\n",
       " ('Very', 7.225632092655587e-09),\n",
       " ('D', 7.213679431572473e-09),\n",
       " ('music', 7.203312613057733e-09),\n",
       " ('No', 7.07412750600156e-09),\n",
       " ('AT', 7.063799323248077e-09),\n",
       " ('Angel', 7.006369262541057e-09),\n",
       " ('Animated', 6.970553023677439e-09),\n",
       " ('Age', 6.967642018906872e-09),\n",
       " ('soon', 6.963948528948549e-09),\n",
       " ('##d', 6.932142859739088e-09),\n",
       " ('3', 6.814537378829755e-09),\n",
       " ('##2', 6.798814844444223e-09),\n",
       " ('whom', 6.79375933287929e-09),\n",
       " ('1950', 6.736250224292917e-09),\n",
       " ('BC', 6.735775048838377e-09),\n",
       " ('ca', 6.721772916051805e-09),\n",
       " ('Administration', 6.690103582229767e-09),\n",
       " ('##va', 6.603440017016737e-09),\n",
       " ('May', 6.5418244155068805e-09),\n",
       " ('Dr', 6.541100550094825e-09),\n",
       " ('Then', 6.507960836898974e-09),\n",
       " ('events', 6.455566747831654e-09),\n",
       " ('than', 6.377923966738308e-09),\n",
       " ('animation', 6.349630155000341e-09),\n",
       " ('b', 6.344739400532262e-09),\n",
       " ('Max', 6.326842605375305e-09),\n",
       " ('Publishing', 6.2944760514938025e-09),\n",
       " ('Aviation', 6.279953890242496e-09),\n",
       " ('Card', 6.279354813898408e-09),\n",
       " ('de', 6.277558473044564e-09),\n",
       " ('##ch', 6.26611029730384e-09),\n",
       " ('Movement', 6.250485018455265e-09),\n",
       " ('Too', 6.207962144344492e-09),\n",
       " ('Sign', 6.205759905952846e-09),\n",
       " ('School', 6.154883269715583e-09),\n",
       " ('1928', 6.1087099822998425e-09),\n",
       " ('Earth', 6.047464307101791e-09),\n",
       " ('any', 6.0342140173474945e-09),\n",
       " ('Each', 6.003459063208538e-09),\n",
       " ('Meyer', 6.000608454570511e-09),\n",
       " ('Group', 5.994556850907884e-09),\n",
       " ('forever', 5.9924079032214195e-09),\n",
       " ('Era', 5.985611117864664e-09),\n",
       " ('along', 5.951878989662873e-09),\n",
       " ('K', 5.948553649659516e-09),\n",
       " ('usually', 5.92096771612205e-09),\n",
       " ('within', 5.9078595349149055e-09),\n",
       " ('Theater', 5.899853050550519e-09),\n",
       " ('Gracie', 5.899774002671165e-09),\n",
       " ('civilization', 5.826081839188646e-09),\n",
       " ('Santa', 5.812795134119142e-09),\n",
       " ('As', 5.79097259034711e-09),\n",
       " ('Aero', 5.76529401996595e-09),\n",
       " ('##ri', 5.755010246133452e-09),\n",
       " ('n', 5.751707554679797e-09),\n",
       " ('Alexander', 5.6687272653732634e-09),\n",
       " ('##ph', 5.630645283360991e-09),\n",
       " ('Du', 5.58325874422394e-09),\n",
       " ('museum', 5.582385664837375e-09),\n",
       " ('Paris', 5.580075512767735e-09),\n",
       " ('1945', 5.575288231085551e-09),\n",
       " ('somewhere', 5.551201276432494e-09),\n",
       " ('Jack', 5.5297491030614765e-09),\n",
       " ('Street', 5.5007260968409355e-09),\n",
       " ('want', 5.49899503710094e-09),\n",
       " ('society', 5.496855859377092e-09),\n",
       " ('Who', 5.461917584881348e-09),\n",
       " ('King', 5.452810203365743e-09),\n",
       " ('Theo', 5.445492945455044e-09),\n",
       " ('once', 5.433468341919934e-09),\n",
       " ('Turn', 5.421676885219995e-09),\n",
       " ('##ude', 5.415889070548019e-09),\n",
       " ('even', 5.4115010250654905e-09),\n",
       " ('India', 5.409642955811478e-09),\n",
       " ('York', 5.409591441463135e-09),\n",
       " ('##ª', 5.388809842798992e-09),\n",
       " ('go', 5.337021047324697e-09)]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = LMHeadModel('bert-base-cased')\n",
    "model.get_next_word_probabilities(sentence1, top_k=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-03T00:22:13.781489Z",
     "iopub.status.busy": "2024-10-03T00:22:13.780873Z",
     "iopub.status.idle": "2024-10-03T00:22:13.787569Z",
     "shell.execute_reply": "2024-10-03T00:22:13.786401Z",
     "shell.execute_reply.started": "2024-10-03T00:22:13.781458Z"
    }
   },
   "outputs": [],
   "source": [
    "class LLM_model:\n",
    "    def __init__(self, name, size, with_prompt = False):\n",
    "        self.name = name\n",
    "        self.max_input_len = size\n",
    "        self.with_prompt = with_prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split Training Text into Segments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-03T00:22:13.790119Z",
     "iopub.status.busy": "2024-10-03T00:22:13.789396Z",
     "iopub.status.idle": "2024-10-03T00:22:13.946533Z",
     "shell.execute_reply": "2024-10-03T00:22:13.945285Z",
     "shell.execute_reply.started": "2024-10-03T00:22:13.790065Z"
    }
   },
   "outputs": [],
   "source": [
    "#Generates a random integer partition of (a, b)\n",
    "\n",
    "def rand_part(a, b):\n",
    "    part = []\n",
    "    x = randint(a, b)\n",
    "    part += min(x, b)\n",
    "    if part[-1]==b:\n",
    "        return part\n",
    "    else:\n",
    "        part.extend(rand_part(x, b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-03T00:22:13.948537Z",
     "iopub.status.busy": "2024-10-03T00:22:13.948058Z",
     "iopub.status.idle": "2024-10-03T00:22:13.959556Z",
     "shell.execute_reply": "2024-10-03T00:22:13.958203Z",
     "shell.execute_reply.started": "2024-10-03T00:22:13.948496Z"
    }
   },
   "outputs": [],
   "source": [
    "import random\n",
    "def split_txt(begin, essay, max_seq_len = 512):\n",
    "    segments = []\n",
    "    y = min(len(essay)-1, begin + max_seq_len - 1)\n",
    "    x = random.randint(begin, y)\n",
    "    if essay[x] == ' ':\n",
    "        #segments.append(essay[:x])\n",
    "        segments.append(x)\n",
    "    else:\n",
    "        while x < y and essay[x] != ' ':\n",
    "                x+=1\n",
    "        if x == y and segments == []:\n",
    "            while essay[x-1] != ' ':\n",
    "                x-=1\n",
    "            #segments.append(essay[:x-1])\n",
    "            segments.append(x-1)\n",
    "            return segments\n",
    "        if x == y and segments != []:\n",
    "            return segments\n",
    "        else:\n",
    "            #segments.append(essay[:x])\n",
    "            segments.append(x)\n",
    "    segments.extend(split_txt(x+1, essay))\n",
    "    return segments\n",
    "        \n",
    "                "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-03T00:22:13.961581Z",
     "iopub.status.busy": "2024-10-03T00:22:13.961087Z",
     "iopub.status.idle": "2024-10-03T00:22:13.973543Z",
     "shell.execute_reply": "2024-10-03T00:22:13.972414Z",
     "shell.execute_reply.started": "2024-10-03T00:22:13.961540Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[487,\n",
       " 799,\n",
       " 812,\n",
       " 972,\n",
       " 1312,\n",
       " 1485,\n",
       " 1511,\n",
       " 1800,\n",
       " 1994,\n",
       " 2234,\n",
       " 2736,\n",
       " 3137,\n",
       " 3152,\n",
       " 3244,\n",
       " 3260,\n",
       " 3282,\n",
       " 3285,\n",
       " 3285]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "split_txt(0, kaggle_train['text'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-03T00:22:13.976018Z",
     "iopub.status.busy": "2024-10-03T00:22:13.975014Z",
     "iopub.status.idle": "2024-10-03T00:22:13.985156Z",
     "shell.execute_reply": "2024-10-03T00:22:13.984189Z",
     "shell.execute_reply.started": "2024-10-03T00:22:13.975971Z"
    }
   },
   "outputs": [],
   "source": [
    "#what's the actual next word in the essay\n",
    "\n",
    "from string import punctuation\n",
    "\n",
    "def true_next_word(essay, n):\n",
    "    word= ''\n",
    "    i = n\n",
    "    while str.isalpha(essay[i]) == False and str.isnumeric(essay[i]) == False:\n",
    "        if essay[i] in punctuation:\n",
    "            word+=essay[i]\n",
    "            return word\n",
    "        else:\n",
    "            i+=1\n",
    "    while essay[i] != ' ' and not(essay[i] in punctuation):\n",
    "        word+=essay[i]\n",
    "        i+=1\n",
    "    return word"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-03T00:22:13.986929Z",
     "iopub.status.busy": "2024-10-03T00:22:13.986562Z",
     "iopub.status.idle": "2024-10-03T00:22:13.999839Z",
     "shell.execute_reply": "2024-10-03T00:22:13.998839Z",
     "shell.execute_reply.started": "2024-10-03T00:22:13.986900Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1900s'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "true_next_word(kaggle_train['text'][0], 59)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-03T00:22:14.001451Z",
     "iopub.status.busy": "2024-10-03T00:22:14.001131Z",
     "iopub.status.idle": "2024-10-03T00:22:14.013160Z",
     "shell.execute_reply": "2024-10-03T00:22:14.012119Z",
     "shell.execute_reply.started": "2024-10-03T00:22:14.001424Z"
    }
   },
   "outputs": [],
   "source": [
    "#homemade 'return index of element if it exists' function\n",
    "\n",
    "def return_index(element, list):\n",
    "    i=0\n",
    "    while i < len(list):\n",
    "        if list[i] == element:\n",
    "            return i\n",
    "        else:\n",
    "            i+=1\n",
    "    if i == len(list):\n",
    "        return -10000\n",
    "    \n",
    "#get all rth elements of a list of tuples\n",
    "\n",
    "def rths(r, list):\n",
    "    rths = []\n",
    "    for i in range(len(list)):\n",
    "        if type(list[i]) is tuple:\n",
    "            if len(list[i]) > r:\n",
    "                rths.append(list[i][r])\n",
    "            else:\n",
    "                rths.append('')\n",
    "        else:\n",
    "            rths.append('')\n",
    "    return rths\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prediction with prompt and source text using GPT2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-03T00:22:14.015099Z",
     "iopub.status.busy": "2024-10-03T00:22:14.014738Z",
     "iopub.status.idle": "2024-10-03T00:22:14.034240Z",
     "shell.execute_reply": "2024-10-03T00:22:14.032976Z",
     "shell.execute_reply.started": "2024-10-03T00:22:14.015069Z"
    }
   },
   "outputs": [],
   "source": [
    "essay_response = \"\"\"\n",
    "In recent years, there has been a notable shift in urban planning, with an increasing focus on limiting car usage as a means to foster sustainable and environmentally friendly communities. This shift is evident in various parts of the world, as seen in the case of Vauban, Germany, where an experimental car-free community has thrived since its completion in 2006 (Rosenthal, 2009). Vauban's success challenges the conventional reliance on cars in suburban areas and serves as a model for smart planning that is gaining traction globally.\n",
    "\n",
    "Vauban's innovative approach to urban development is part of a broader movement to reduce the environmental impact of cars, particularly in suburban settings where car-centric lifestyles have long been the norm. According to experts, passenger cars contribute significantly to greenhouse gas emissions, with Europe attributing 12 percent of emissions to this source, and in some car-intensive areas in the United States, the contribution climbs to a staggering 50 percent (Rosenthal, 2009). Recognizing the environmental implications, planners worldwide are reimagining suburbs, moving away from the traditional car-centric model.\n",
    "\n",
    "One significant aspect of the shift in urban planning is the concept of \"smart planning,\" where suburbs are designed to be more compact and accessible to public transportation, reducing the need for extensive parking spaces (Rosenthal, 2009). The Vauban model encourages walking and cycling, with essential amenities placed within walking distance along main streets, challenging the conventional suburban sprawl and promoting a more sustainable lifestyle.\n",
    "\n",
    "The movement toward limiting car usage is not limited to Europe. In Paris, the detrimental impact of car emissions on air quality led to the implementation of a partial driving ban during periods of intense smog (Duffer, 2014). The success of such initiatives is evident in the significant reduction of congestion and improvement in air quality. Similarly, Bogota, Colombia, has embraced a car-free day annually, encouraging alternative transportation methods and reducing both traffic jams and smog levels (Selsky, 2002). The success of these initiatives underscores the potential benefits of limiting car usage in diverse urban settings.\n",
    "\n",
    "In the United States, there is a growing awareness of the need to reduce car dependency. Recent studies suggest that Americans are buying fewer cars and driving less, indicating a potential shift in cultural attitudes toward car ownership (Rosenthal, 2013). This change aligns with efforts to decrease carbon emissions, as transportation remains a major contributor to the nation's environmental footprint.\n",
    "\n",
    "While the trend toward limiting car usage presents challenges for the traditional automotive industry, it also opens avenues for innovation and adaptation. Companies like Ford and Mercedes are rebranding themselves as \"mobility\" companies, recognizing the evolving needs and preferences of consumers (Rosenthal, 2013). The younger generation, in particular, shows a reduced interest in car ownership, preferring alternative modes of transportation facilitated by technological advancements such as car-sharing programs and ride-sharing apps.\n",
    "\n",
    "In conclusion, the advantages of limiting car usage extend beyond environmental benefits to encompass improved urban planning, reduced congestion, and a shift toward more sustainable lifestyles. The success stories of car-free communities in Germany, driving bans in Paris during smog episodes, and annual car-free days in Bogota demonstrate the feasibility and positive outcomes of such initiatives. As the world grapples with the environmental impact of car culture, embracing alternative transportation models becomes imperative for creating healthier, more livable communities.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-03T00:22:14.036289Z",
     "iopub.status.busy": "2024-10-03T00:22:14.035871Z",
     "iopub.status.idle": "2024-10-03T00:22:14.049681Z",
     "shell.execute_reply": "2024-10-03T00:22:14.048594Z",
     "shell.execute_reply.started": "2024-10-03T00:22:14.036255Z"
    }
   },
   "outputs": [],
   "source": [
    "model1 = LLM_model('gpt2', 4096, with_prompt = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-03T00:22:14.051595Z",
     "iopub.status.busy": "2024-10-03T00:22:14.051258Z",
     "iopub.status.idle": "2024-10-03T00:22:14.062608Z",
     "shell.execute_reply": "2024-10-03T00:22:14.061404Z",
     "shell.execute_reply.started": "2024-10-03T00:22:14.051569Z"
    }
   },
   "outputs": [],
   "source": [
    "def predict_w_prompt(text, prompt, ai):\n",
    "    split_text = split_txt(0, text, ai.max_input_len)\n",
    "    probability = 0\n",
    "    model = LMHeadModel(f'{ai.name}')\n",
    "    for splice in split_text:\n",
    "        if ai.with_prompt:\n",
    "            feed = prompt + text[:splice]\n",
    "        else:\n",
    "            feed = text[:splice]\n",
    "        p_words = model.get_next_word_probabilities(feed[max(0, len(feed)-ai.max_input_len -1):], top_k=500)\n",
    "        index = return_index(true_next_word(text, splice), rths(0, p_words))\n",
    "        print(f'index: {index}')\n",
    "        if index >= 0:\n",
    "            probability += p_words[index][1]\n",
    "            print(f'value: {p_words[index][1]}')\n",
    "        else:\n",
    "            probability += 1e-10\n",
    "        print(f'probability: {probability}')\n",
    "    output = probability/len(split_text)\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-03T00:22:14.067308Z",
     "iopub.status.busy": "2024-10-03T00:22:14.066936Z",
     "iopub.status.idle": "2024-10-03T00:22:14.076788Z",
     "shell.execute_reply": "2024-10-03T00:22:14.075651Z",
     "shell.execute_reply.started": "2024-10-03T00:22:14.067273Z"
    }
   },
   "outputs": [],
   "source": [
    "prompt = kaggle_prompts['source_text'][0]+kaggle_prompts['instructions'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-03T00:22:14.078461Z",
     "iopub.status.busy": "2024-10-03T00:22:14.078098Z",
     "iopub.status.idle": "2024-10-03T00:22:42.503275Z",
     "shell.execute_reply": "2024-10-03T00:22:42.502151Z",
     "shell.execute_reply.started": "2024-10-03T00:22:14.078432Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ff754e7ba88e41df9cf1cc3e496a4485",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading config.json:   0%|          | 0.00/665 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "df59663ffa124297b69e7869d594c5dc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading model.safetensors:   0%|          | 0.00/548M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b9f16d47c2854d718077255e03ce7a7d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading generation_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8ddc114d1b064aeeba404f5b2390c7e0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading tokenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "583bb6a6abe24816a891b20dfe39c69e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading vocab.json:   0%|          | 0.00/1.04M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "15d2320d8eed4738856ca2686a70f36e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "56bd8875686345bd874c862e1d8e522a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index: 1\n",
      "value: 0.35049864649772644\n",
      "probability: 0.35049864649772644\n",
      "index: 2\n",
      "value: 0.1668398082256317\n",
      "probability: 0.5173384547233582\n",
      "index: 0\n",
      "value: 0.9775412678718567\n",
      "probability: 1.4948797225952148\n",
      "index: 17\n",
      "value: 0.00487076910212636\n",
      "probability: 1.4997504916973412\n",
      "index: 78\n",
      "value: 0.0016030677361413836\n",
      "probability: 1.5013535594334826\n",
      "index: 0\n",
      "value: 0.9717299938201904\n",
      "probability: 2.473083553253673\n",
      "index: 33\n",
      "value: 0.0020588389597833157\n",
      "probability: 2.4751423922134563\n",
      "index: 0\n",
      "value: 0.712573230266571\n",
      "probability: 3.1877156224800274\n",
      "index: 1\n",
      "value: 0.2337258756160736\n",
      "probability: 3.421441498096101\n",
      "index: 1\n",
      "value: 0.2337258756160736\n",
      "probability: 3.6551673737121746\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.3655167373712175"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_w_prompt(essay_response, prompt, model1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-03T00:22:42.505384Z",
     "iopub.status.busy": "2024-10-03T00:22:42.504928Z",
     "iopub.status.idle": "2024-10-03T00:22:42.511072Z",
     "shell.execute_reply": "2024-10-03T00:22:42.510084Z",
     "shell.execute_reply.started": "2024-10-03T00:22:42.505343Z"
    }
   },
   "outputs": [],
   "source": [
    "model0 = LLM_model('gpt2', 4096, with_prompt = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-03T00:22:42.513346Z",
     "iopub.status.busy": "2024-10-03T00:22:42.512493Z",
     "iopub.status.idle": "2024-10-03T00:22:59.022923Z",
     "shell.execute_reply": "2024-10-03T00:22:59.021919Z",
     "shell.execute_reply.started": "2024-10-03T00:22:42.513307Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index: 0\n",
      "value: 0.4849114418029785\n",
      "probability: 0.4849114418029785\n",
      "index: 0\n",
      "value: 0.6759265661239624\n",
      "probability: 1.160838007926941\n",
      "index: 0\n",
      "value: 0.8434841632843018\n",
      "probability: 2.0043221712112427\n",
      "index: 0\n",
      "value: 0.15197552740573883\n",
      "probability: 2.1562976986169815\n",
      "index: 19\n",
      "value: 0.004859927576035261\n",
      "probability: 2.1611576261930168\n",
      "index: 1\n",
      "value: 0.15160542726516724\n",
      "probability: 2.312763053458184\n",
      "index: 12\n",
      "value: 0.0079202214255929\n",
      "probability: 2.320683274883777\n",
      "index: 1\n",
      "value: 0.22824814915657043\n",
      "probability: 2.5489314240403473\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.3186164280050434"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_w_prompt(essay_response, prompt, model0)"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "databundleVersionId": 7516023,
     "sourceId": 61542,
     "sourceType": "competition"
    }
   ],
   "dockerImageVersionId": 30587,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python (miniforge)",
   "language": "python",
   "name": "miniforge"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
